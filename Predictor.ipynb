{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.6.2-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 46 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /home/ashish/anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/ashish/anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.18.2)\n",
      "Requirement already satisfied: scipy>=0.17 in /home/ashish/anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ashish/anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.14.1)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.6.2 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "rindia = pd.read_csv('all_rIndiaScrape2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>author</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>comments_count</th>\n",
       "      <th>crossposts_count</th>\n",
       "      <th>comments</th>\n",
       "      <th>removed_by</th>\n",
       "      <th>self_posted</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>is_stickied</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0lfy8</td>\n",
       "      <td>Late Night Random Discussion Thread !</td>\n",
       "      <td>^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>oxythebot</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>27</td>\n",
       "      <td>2020-04-14 05:15:26</td>\n",
       "      <td>2610</td>\n",
       "      <td>0</td>\n",
       "      <td>Frenchwhorehouse: bruh blurringthough  is gett...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g0lfy8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fx8sbw</td>\n",
       "      <td>Late Night Random Discussion Thread !</td>\n",
       "      <td>^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>oxythebot</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>30</td>\n",
       "      <td>2020-04-09 05:15:11</td>\n",
       "      <td>3362</td>\n",
       "      <td>0</td>\n",
       "      <td>Called two people today, one was the same pers...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fx8sbw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fzpfrc</td>\n",
       "      <td>Random Daily Discussion Thread - April 12, 202...</td>\n",
       "      <td>^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>oxythebot</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>27</td>\n",
       "      <td>2020-04-12 17:15:29</td>\n",
       "      <td>2073</td>\n",
       "      <td>0</td>\n",
       "      <td>One of my all time favorite movie\\n\\nhttps://i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fzpfrc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fzyyju</td>\n",
       "      <td>Late Night Random Discussion Thread !</td>\n",
       "      <td>^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>oxythebot</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>30</td>\n",
       "      <td>2020-04-13 05:15:03</td>\n",
       "      <td>1794</td>\n",
       "      <td>0</td>\n",
       "      <td>Man being alone during this lockdown is soo bo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fzyyju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi8g2</td>\n",
       "      <td>Late Night Random Discussion Thread !</td>\n",
       "      <td>^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>oxythebot</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>29</td>\n",
       "      <td>2020-04-11 05:15:03</td>\n",
       "      <td>2286</td>\n",
       "      <td>0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fyi8g2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  g0lfy8              Late Night Random Discussion Thread !   \n",
       "1  fx8sbw              Late Night Random Discussion Thread !   \n",
       "2  fzpfrc  Random Daily Discussion Thread - April 12, 202...   \n",
       "3  fzyyju              Late Night Random Discussion Thread !   \n",
       "4  fyi8g2              Late Night Random Discussion Thread !   \n",
       "\n",
       "                                             content  is_original_content  \\\n",
       "0  ^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...                False   \n",
       "1  ^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...                False   \n",
       "2  ^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...                False   \n",
       "3  ^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...                False   \n",
       "4  ^Beep ^Boop ^Bot, ^I ^am ^a ^bot! ^if ^any ^pr...                False   \n",
       "\n",
       "      author      flair  score            timestamp  comments_count  \\\n",
       "0  oxythebot  Scheduled     27  2020-04-14 05:15:26            2610   \n",
       "1  oxythebot  Scheduled     30  2020-04-09 05:15:11            3362   \n",
       "2  oxythebot  Scheduled     27  2020-04-12 17:15:29            2073   \n",
       "3  oxythebot  Scheduled     30  2020-04-13 05:15:03            1794   \n",
       "4  oxythebot  Scheduled     29  2020-04-11 05:15:03            2286   \n",
       "\n",
       "   crossposts_count                                           comments  \\\n",
       "0                 0  Frenchwhorehouse: bruh blurringthough  is gett...   \n",
       "1                 0  Called two people today, one was the same pers...   \n",
       "2                 0  One of my all time favorite movie\\n\\nhttps://i...   \n",
       "3                 0  Man being alone during this lockdown is soo bo...   \n",
       "4                 0                                         [deleted]    \n",
       "\n",
       "   removed_by  self_posted   nsfw  is_stickied  \\\n",
       "0         NaN         True  False        False   \n",
       "1         NaN         True  False        False   \n",
       "2         NaN         True  False        False   \n",
       "3         NaN         True  False        False   \n",
       "4         NaN         True  False        False   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/india/comments/g0lfy8...  \n",
       "1  https://www.reddit.com/r/india/comments/fx8sbw...  \n",
       "2  https://www.reddit.com/r/india/comments/fzpfrc...  \n",
       "3  https://www.reddit.com/r/india/comments/fzyyju...  \n",
       "4  https://www.reddit.com/r/india/comments/fyi8g2...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rindia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2878, 16)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rindia.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "rindia.fillna(\"\",inplace = True)\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def to_strings(value):\n",
    "    return str(value)\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = text.replace(\"(<br/>)\", \"\")\n",
    "    text = text.replace('(<a).*(>).*(</a>)', '')\n",
    "    text = text.replace('(&amp)', '')\n",
    "    text = text.replace('(&gt)', '')\n",
    "    text = text.replace('(&lt)', '')\n",
    "    text = text.replace('(\\xa0)', ' ') \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    \n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"What's\", \"what is\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text=re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "    text=text.lower().replace(\"indian\",\"india\")\n",
    "    \n",
    "    return ' '.join(text.split()).replace(r\"\\s*\\([^()]*\\)\",\"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the preprocessing on features like 'title','content','comments' and later generating a synthetic feature by combining all three features + url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashish/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:314: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "rindia['title'] = rindia['title'].apply(to_strings)\n",
    "rindia['content'] = rindia['content'].apply(to_strings)\n",
    "rindia['comments'] = rindia['comments'].apply(to_strings)\n",
    "\n",
    "\n",
    "rindia['title'] = rindia['title'].apply(preprocess)\n",
    "rindia['content'] = rindia['content'].apply(preprocess)\n",
    "rindia['comments'] = rindia['comments'].apply(preprocess)\n",
    "\n",
    "all_features = rindia[\"title\"] + rindia[\"comments\"] + rindia[\"content\"] + rindia[\"url\"]\n",
    "rindia['all_features'] = all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166311"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rindia['all_features'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Scheduled', 'AskIndia', '[R]eddiquette', 'Politics',\n",
       "       'Non-Political', 'Business/Finance', 'Policy/Economy',\n",
       "       'Science/Technology', 'Sports', 'Food', 'AMA', 'Photography'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rindia['flair'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary for the target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_classes = ['Scheduled', 'AskIndia', '[R]eddiquette', 'Politics',\n",
    "       'Non-Political', 'Business/Finance', 'Policy/Economy',\n",
    "       'Science/Technology', 'Sports', 'Food', 'AMA', 'Photography']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rindia.all_features\n",
    "y = rindia['flair']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial NB for Multinomial Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', MultinomialNB()),\n",
    "                ])\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=flairs))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                 ])\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=flair_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "  \n",
    "rf = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', RandomForestClassifier(n_estimators = 1000, random_state = 42)),\n",
    "                 ])\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=flair_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-layer perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "  \n",
    "mlp = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', MLPClassifier(hidden_layer_sizes=(30,30,30))),\n",
    "                 ])\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=flair_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=None,lower=True,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',split=' ',char_level=False)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tokenizer.texts_to_sequences(X_test)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1500\n",
    "max_len = 200\n",
    "sequences_matrix = sequence.pad_sequences(x_train,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# f = open('/media/ashish/Seagate Expansion Drive/Crysagi Systems/glove.6B/glove.6B.100d.txt')\n",
    "# embeddings_index = {}\n",
    "# for line in f:\n",
    "#     val = line.split()\n",
    "#     word = val[0]\n",
    "#     coff = np.asarray(val[1:],dtype = 'float')\n",
    "#     embeddings_index[word] = coff\n",
    "\n",
    "# f.close()\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 100\n",
    "# k = 0\n",
    "# embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # Words not found in embedding index will be all-zeros.\n",
    "#         k += 1\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 200, 200)          300000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_12 (Spatia (None, 200, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 200, 200)          320800    \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 200, 200)          320800    \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 256)               51456     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 12)                396       \n",
      "=================================================================\n",
      "Total params: 1,357,484\n",
      "Trainable params: 1,357,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Sequential LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=sequences_matrix.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add((LSTM(200, dropout=0.2, recurrent_dropout=0.2,return_sequences = True))) \n",
    "model.add((LSTM(200, dropout=0.2, recurrent_dropout=0.2,return_sequences = True))) \n",
    "model.add((LSTM(200, dropout=0.2, recurrent_dropout=0.2))) \n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "# batch_size = 20\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=16, kernel_size=1, padding=\"valid\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 200, 200)          300000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, 200, 200)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 200, 16)           3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 100, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 99, 32)            1056      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 49, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 46, 64)            8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 23, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 18, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 9, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 2, 256)            262400    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 12)                396       \n",
      "=================================================================\n",
      "Total params: 733,628\n",
      "Trainable params: 733,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  \n",
      "/home/ashish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=32, kernel_size=2, padding=\"valid\")`\n",
      "  import sys\n",
      "/home/ashish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  if __name__ == '__main__':\n",
      "/home/ashish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=4, padding=\"valid\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/ashish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  if sys.path[0] == '':\n",
      "/home/ashish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=6, padding=\"valid\")`\n",
      "  del sys.path[0]\n",
      "/home/ashish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ashish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=256, kernel_size=8, padding=\"valid\")`\n",
      "  app.launch_new_instance()\n",
      "/home/ashish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, EMBEDDING_DIM, input_length=sequences_matrix.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Conv1D(nb_filter=16,filter_length=1,border_mode=\"valid\",activation=\"relu\",))\n",
    "# standard max pooling\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Conv1D(nb_filter=32,filter_length=2,border_mode=\"valid\",activation=\"relu\",))\n",
    "# standard max pooling\n",
    "model.add(MaxPooling1D(pool_length=2))  \n",
    "model.add(Conv1D(nb_filter=64,filter_length=4,border_mode=\"valid\",activation=\"relu\",))\n",
    "# standard max pooling\n",
    "model.add(MaxPooling1D(pool_length=2)) \n",
    "model.add(Conv1D(nb_filter=128,filter_length=6,border_mode=\"valid\",activation=\"relu\",))\n",
    "# standard max pooling\n",
    "model.add(MaxPooling1D(pool_length=2)) \n",
    "model.add(Conv1D(nb_filter=256,filter_length=8,border_mode=\"valid\",activation=\"relu\",))\n",
    "# standard max pooling\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=15, batch_size=15,validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Model\n",
    "import pickle\n",
    "filename = 'CNN_model_final.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
